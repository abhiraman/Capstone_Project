{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M/C_Transliteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO54WbPPxEvIBReytnu+hYc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiraman/Capstone_Project/blob/main/M_C_Transliteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tvoTXVpJeVN"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFVwbpaYJdl1"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import string,re\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVu7OSzhDREV"
      },
      "source": [
        "# Load Data from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY-Im6k5DLQM",
        "outputId": "034aa559-ce06-432c-c811-d012fe3538cf"
      },
      "source": [
        "!git clone -l -s git://github.com/GokulNC/NLP-Exercises cloned-repo"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Total 72 (delta 0), reused 0 (delta 0), pack-reused 72\u001b[K\n",
            "Receiving objects: 100% (72/72), 2.39 MiB | 21.46 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5LVtrQNHyFP",
        "outputId": "a53ce1fc-8fe3-4628-a4d4-b7a4079275d8"
      },
      "source": [
        "%cd cloned-repo"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-repo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa01lfLNIBbu",
        "outputId": "9209909d-eb2c-4ea5-af78-2749a8cca469"
      },
      "source": [
        "%cd Transliteration-Indian-Languages/Original-NEWS2012-data/Training\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-repo/Transliteration-Indian-Languages/Original-NEWS2012-data/Training\n",
            "NEWS2012-Training-EnBa-14623.xml  NEWS2012-Training-EnKa-11955.xml\n",
            "NEWS2012-Training-EnHe-11501.xml  NEWS2012-Training-EnMa-9000.xml\n",
            "NEWS2012-Training-EnHi-13937.xml  NEWS2012-Training-EnTa-11957.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq7BP0iIZy_u",
        "outputId": "d6bcba5a-e734-4ecf-f098-328945ce4543"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  MyDevice = 'cuda'\n",
        "else:MyDevice = 'cpu'\n",
        "print(MyDevice)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTyZiCO9xEZE"
      },
      "source": [
        "Getting all Hindi & English letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq2ssxlxa4IZ",
        "outputId": "56ae89b1-38b1-46c2-b60d-7ae1fc7ff11c"
      },
      "source": [
        "## Get all hindi consonants ##\n",
        "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "pad = \"PAD\"\n",
        "hindi_alphabets = [pad]+[chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabets_indexed = {hindi_alphabets[i]:i for i in range(len(hindi_alphabets))}\n",
        "print(hindi_alphabets_indexed)\n",
        "\n",
        "english_alphabets = string.ascii_uppercase\n",
        "english_alphabets_indexed = {}\n",
        "english_alphabets_indexed[pad]=0\n",
        "for ind,char in enumerate(english_alphabets,start=1):\n",
        "  english_alphabets_indexed[char] = ind\n",
        "print(len(english_alphabets_indexed))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'PAD': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF41Hul2xMhk"
      },
      "source": [
        "Clean String Lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI3hfqWgerAR"
      },
      "source": [
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "def _cleanEnglishWord(line):\n",
        "  line = line.replace('-',' ').replace(',',' ').upper()\n",
        "  line = non_eng_letters_regex.sub('', line)\n",
        "  return line.split()\n",
        "\n",
        "def _cleanLanguageWord(line):\n",
        "  line = line.replace('-',' ').replace(',',' ')\n",
        "  cleanedStr = ''\n",
        "  for eChar in line:\n",
        "    if eChar in  hindi_alphabets or eChar in ' ':\n",
        "      cleanedStr+=eChar\n",
        "  return cleanedStr.split()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRuvAaLK9JUQ"
      },
      "source": [
        "# Custom Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF2DbN8AIOgN"
      },
      "source": [
        "class TextLoader(Dataset):\n",
        "  def __init__(self,xmlFile=None):\n",
        "    super().__init__()\n",
        "    self.fileName = xmlFile\n",
        "    self.allEngWords,self.allHindiWords = [],[]\n",
        "    self._read_clean_data()\n",
        "    self.shuffleIndices = list(range(len(self.allEngWords)))\n",
        "    random.shuffle(self.shuffleIndices)\n",
        "    self.startIndex = 0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.allEngWords)\n",
        "\n",
        "  def _read_clean_data(self):\n",
        "    tree = ET.parse(self.fileName)\n",
        "    root = tree.getroot()\n",
        "    for child in root:\n",
        "      engWord = _cleanEnglishWord(child[0].text)\n",
        "      hindWord = _cleanLanguageWord(child[1].text)\n",
        "      if len(engWord)!=len(hindWord):\n",
        "        print(\"Skipping --> {} --- {}\".format(child[0].text,child[1].text))\n",
        "      for eWord in engWord:\n",
        "        self.allEngWords.append(eWord)\n",
        "      for eWord in hindWord:\n",
        "        self.allHindiWords.append(eWord)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return {\"EnglishWord\":self.allEngWords[idx],\"HindiWord\":self.allHindiWords[idx]}\n",
        "  \n",
        "  def _get_batch_words(self,batchSize,array):\n",
        "    end= self.startIndex + batchSize\n",
        "    batch = []\n",
        "    return batch + [array[self.shuffleIndices[i]] for i in range(end)]\n",
        "  \n",
        "  def _return_batch_words(self,batchSize):\n",
        "    engWords = self._get_batch_words(batchSize,self.allEngWords)\n",
        "    hindiWords = self._get_batch_words(batchSize,self.allHindiWords)\n",
        "    return engWords,hindiWords\n",
        "\n",
        "dataSet = TextLoader(xmlFile='NEWS2012-Training-EnHi-13937.xml')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y3ai8-CJpEK",
        "outputId": "6f8605b8-a37d-4c13-cba1-9df3b51a4a16"
      },
      "source": [
        "for ind,i in enumerate(dataSet):\n",
        "  if ind>5:break\n",
        "  print(i)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'EnglishWord': 'RAASAVIHAAREE', 'HindiWord': 'रासविहारी'}\n",
            "{'EnglishWord': 'DEOGAN', 'HindiWord': 'देवगन'}\n",
            "{'EnglishWord': 'ROAD', 'HindiWord': 'रोड'}\n",
            "{'EnglishWord': 'SHATRUMARDAN', 'HindiWord': 'शत्रुमर्दन'}\n",
            "{'EnglishWord': 'MAHIJUBA', 'HindiWord': 'महिजुबा'}\n",
            "{'EnglishWord': 'SABINE', 'HindiWord': 'सैबिन'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAMgSe9QPfjp"
      },
      "source": [
        "def word_rep(word, letter2index, device = 'cpu'):\n",
        "    rep = torch.zeros(len(word)+1, 1, len(letter2index)).to(device)\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        rep[letter_index][0][pos] = 1\n",
        "    pad_pos = letter2index[\"PAD\"]\n",
        "    rep[letter_index+1][0][pad_pos] = 1\n",
        "    return rep\n",
        "\n",
        "def gt_rep(word, letter2index, device = 'cpu'):\n",
        "    gt_rep = torch.zeros([len(word)+1, 1], dtype=torch.long).to(device)\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        gt_rep[letter_index][0] = pos\n",
        "    gt_rep[letter_index+1][0] = letter2index[\"PAD\"]\n",
        "    return gt_rep"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRGKOd1JCBfj"
      },
      "source": [
        "Enoder Decoder W/O Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCxj2j59R-H"
      },
      "source": [
        "class Encoder_Decoder(nn.Module):\n",
        "  def __init__(self,inputSize,hiddenSize,outputSize,num_layers =1,num_dirns=1,verbose=True):\n",
        "    super().__init__()\n",
        "    self.hiddenSize = hiddenSize\n",
        "    self.outputSize = outputSize\n",
        "    self.num_layers = num_layers\n",
        "    self.num_dirns = num_dirns\n",
        "    self.encoder_GRU = nn.GRU(inputSize,hiddenSize)\n",
        "    self.decoder_GRU = nn.GRU(outputSize,hiddenSize)\n",
        "    self.h2o = nn.Linear(hiddenSize,outputSize)\n",
        "    self.F = nn.LogSoftmax(dim=2)\n",
        "    self.Fll = nn.Softmax(dim=2)\n",
        "    self.verbose = verbose\n",
        "  \n",
        "  def forward(self,inputs,maxCharLen,GT=None,trainFlag =True,device='cpu'):\n",
        "    all_hidden,last_hidden = self.encoder_GRU (inputs)\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"Encoder Input : \",inputs.size())\n",
        "      print(\"Encoder All Hidden Outputs : \",all_hidden.size())\n",
        "      print(\"Encoder Last Hidden Output : \",last_hidden.size())\n",
        "\n",
        "\n",
        "    decoder_state = last_hidden\n",
        "    decoderInput = torch.zeros(1,all_hidden.size()[1],self.outputSize).to(device) ##(1,batchSize,no.of English Alphabets)\n",
        "    if self.verbose:\n",
        "      print(\"Decoder Input : \",decoderInput.size())\n",
        "    \n",
        "\n",
        "    outputlist = []   \n",
        "    for i in range(maxCharLen):\n",
        "      out,decoder_state = self.decoder_GRU(decoderInput,decoder_state)\n",
        "      output = self.h2o(decoder_state)\n",
        "      output = self.F(output)\n",
        "\n",
        "      if trainFlag:\n",
        "        outputlist.append(output.view(1, -1))\n",
        "      else:\n",
        "        output_eval = self.Fll(self.h2o(out))\n",
        "        outputlist.append(output_eval.squeeze(0))\n",
        "      if self.verbose:\n",
        "        print(\"Decoder Ouput : \",output.size())\n",
        "        print(\"Squeezed Final Output : \",output.squeeze(0).size())\n",
        "        \n",
        "\n",
        "      maxIndexes = torch.argmax(output,dim=2,keepdim=True).type(torch.int64)\n",
        "      if GT!=None:\n",
        "        maxIndexes = GT[i].reshape(1, 1, 1)\n",
        "\n",
        "      one_hot = torch.FloatTensor(output.size()).to(device)\n",
        "      one_hot.zero_()\n",
        "      one_hot.scatter_(2,maxIndexes,1)\n",
        "      decoderInput = one_hot.detach()\n",
        "    return outputlist\n",
        "      \n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UpaT0WOH9Ey"
      },
      "source": [
        "def trainBatch(modelObj,batchSize,optFn,LossFn,enforceTrain=False,device='cpu'):\n",
        "  textLoaderObj = TextLoader(xmlFile='NEWS2012-Training-EnHi-13937.xml')\n",
        "  engList,hindiList = textLoaderObj._return_batch_words(batchSize)\n",
        "\n",
        "  total_loss = 0\n",
        "  batch_counter = 0\n",
        " \n",
        "  for eWord,hWord in zip(engList,hindiList):\n",
        "    inputs,targets = word_rep(hWord,hindi_alphabets_indexed,device=device),gt_rep(eWord,english_alphabets_indexed,device=device)\n",
        "    pred_ouputs = modelObj(inputs,len(targets),GT= targets if enforceTrain else None,device=device)\n",
        "    ## Loss & Gradient compute for evry time step ##\n",
        "    for index,ouputs in enumerate(pred_ouputs):\n",
        "      loss = LossFn(ouputs,targets[index])/batchSize ## Loss per word\n",
        "      loss.backward(retain_graph=True)\n",
        "      total_loss+=loss.item()        ## Total Loss per batch\n",
        "  \n",
        "\n",
        "  return total_loss/batchSize   ## Total Loss per Epoch\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxSd_pnaGt12"
      },
      "source": [
        "def training_helper(net,lr=0.5,batch_size=100,epochs=11,momentum = 0.9, display_freq=5, device = 'cpu'):\n",
        "  net.to(device)\n",
        "  lossFn = nn.NLLLoss(ignore_index = -1)\n",
        "  optFn = optim.Adam(net.parameters(),lr=lr)\n",
        "  enforce_Till = epochs//3\n",
        "  sheduler = optim.lr_scheduler.StepLR(optFn,step_size=500,gamma=0.5)\n",
        "  \n",
        "\n",
        "\n",
        "  loss_per_epoch_array = torch.zeros(epochs+1)\n",
        "  minVal= 1000000\n",
        "  for i in range(epochs):\n",
        "    optFn.zero_grad()\n",
        "    loss_per_epoch_array[i+1] = (loss_per_epoch_array[i]*i + trainBatch(net, batch_size,optFn, lossFn, device = device, enforceTrain=True if i<enforce_Till else False ))/(i + 1)\n",
        "    optFn.step()\n",
        "    sheduler.step()\n",
        "    \n",
        "    if sheduler.get_lr()!=sheduler.get_last_lr():\n",
        "      print(sheduler.get_lr())\n",
        "\n",
        "    if loss_per_epoch_array[i]<minVal and i>0:\n",
        "\n",
        "      minVal = loss_per_epoch_array[i]\n",
        "      torch.save(net.state_dict(),'model_attention.pt')\n",
        "\n",
        "    if i%display_freq == 0 and i!=0: ## Every 5 epochs refresh the loss plot ##\n",
        "      clear_output(wait=True)\n",
        "      print(\"For Epoch {} ----> Loss {}\".format(i,loss_per_epoch_array[i]))\n",
        "      print(minVal)\n",
        "      plt.figure()\n",
        "      plt.plot(loss_per_epoch_array[1:i],'-*')\n",
        "      plt.xlabel(\"Epochs\")\n",
        "      plt.ylabel(\"Epoch Loss\")\n",
        "      plt.show()\n",
        "  return loss_per_epoch_array\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7cucnEpHev0"
      },
      "source": [
        "## HyperParameters ##\n",
        "hiddensize = 256\n",
        "lr = 0.005\n",
        "momentum = 0.9\n",
        "batch_size=64"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VJqSyWSGrRP"
      },
      "source": [
        "modelObj = Encoder_Decoder(len(hindi_alphabets_indexed),hiddensize,len(english_alphabets_indexed),verbose=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5_9rS9Z0jRm"
      },
      "source": [
        "# Encoder Decoder With Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qtj6is6rLtS"
      },
      "source": [
        "class Encoder_Decoder_with_Attention(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size,device='cpu',verbose=False):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.encoder = nn.GRU(input_size,hidden_size)\n",
        "    self.decoder = nn.GRU(hidden_size*2,hidden_size)\n",
        "\n",
        "    self.h2o = nn.Linear(hidden_size,output_size)    \n",
        "    self.softMAX = nn.LogSoftmax(dim=2)\n",
        "    self.Fll = nn.Softmax(dim=2)\n",
        "    self.TaNh = nn.Tanh()\n",
        "\n",
        "    self.W = nn.Linear(self.hidden_size,self.hidden_size)\n",
        "    self.U = nn.Linear(self.hidden_size,self.hidden_size)\n",
        "    self.attn = nn.Linear(self.hidden_size,1)\n",
        "\n",
        "    self.outTohidden = nn.Linear(self.output_size,self.hidden_size)\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def forward(self,input,targetLen,GT=None,trainFlag=True,device='cpu'):\n",
        "    encoder_output,lastHidden = self.encoder(input)\n",
        "    encoder_output = encoder_output.permute(1,0,2)\n",
        "    if self.verbose:\n",
        "      print(\"Endcoder Input : \",input.size())\n",
        "      print('Encoder all time step hidden states : ',encoder_output.size())\n",
        "      print('Encoder Last hidden States : ',lastHidden.size())\n",
        "    decoderState = lastHidden\n",
        "    decoderInput = torch.zeros(1,1,self.output_size).to(device)\n",
        "\n",
        "    U = self.U(encoder_output) ## (no.of time steps,bathc Size,hidden size)\n",
        "    outputlist = []\n",
        "    for i in range(targetLen):\n",
        "      \n",
        "      W = self.W(decoderState).repeat(1,1*U.size()[1],1) ## (no.of time steps,batch Size,hidden size)\n",
        "      attn = self.attn(self.TaNh(U+W)).permute(0,2,1) ## (1,1,no.of time steps)\n",
        "      attn_applied = torch.bmm(attn,encoder_output) ## (1,1,hidden size)\n",
        "\n",
        "      embedding = self.outTohidden(decoderInput) ## (1,1,hidden size)\n",
        "      out,decoderState = self.decoder(torch.cat([embedding,attn_applied],dim=2))\n",
        "      output = self.softMAX(self.h2o(decoderState))\n",
        "\n",
        "      if self.verbose:\n",
        "        print(\"U*Encoder all Time step hidden states : \",U.size())\n",
        "        print(\"W*Decoder Previous hidden state : \",W.size())\n",
        "        print(\"attn Fynction output : \",attn.size())\n",
        "        print('Attention Applied : ',attn_applied.size())\n",
        "        print(\"Previous Character Embedding : \",embedding.size())\n",
        "        print(\"Intermediate Decoder Output : \",out.size())\n",
        "        print(\"Decoder final Output : \",output.size())\n",
        "\n",
        "      if trainFlag:\n",
        "        outputlist.append(output.view(1, -1))\n",
        "      else:\n",
        "        output_eval = self.Fll(self.h2o(out))\n",
        "        outputlist.append(output_eval.squeeze(0))\n",
        "        \n",
        "      maxIndexes = torch.argmax(output,dim=2,keepdim=True).type(torch.int64)\n",
        "      if GT!=None:\n",
        "        maxIndexes = GT[i].reshape(1, 1, 1)\n",
        "\n",
        "      one_hot = torch.FloatTensor(output.size()).to(device)\n",
        "      one_hot.zero_()\n",
        "      one_hot.scatter_(2,maxIndexes,1)\n",
        "      decoderInput = one_hot.detach()\n",
        "\n",
        "    return outputlist\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2aVv3ZA2S6n"
      },
      "source": [
        "modAttentionObj = Encoder_Decoder_with_Attention(len(hindi_alphabets_indexed),256,len(english_alphabets_indexed),device=MyDevice,verbose=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOwcGCQtLf5R"
      },
      "source": [
        "#training_helper(modAttentionObj,lr=lr, momentum = momentum,batch_size=batch_size,epochs=2500,display_freq=10,device=MyDevice)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow2qfeaD5Tn4"
      },
      "source": [
        "#training_helper(modelObj,lr=lr, momentum = momentum,batch_size=batch_size,epochs=1000,display_freq=10,device=MyDevice)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpGXYOUurtlF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQranJmZr4JC",
        "outputId": "fd74e9be-c7b8-4547-aeac-4d60dab846c3"
      },
      "source": [
        "%cd /gdrive/MyDrive/Capstone_project_data/cloned-repo/Transliteration-Indian-Languages/Original-NEWS2012-data/Training\n",
        "!ls"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Capstone_project_data/cloned-repo/Transliteration-Indian-Languages/Original-NEWS2012-data/Training\n",
            "model_attention.pt\t\t  NEWS2012-Training-EnHi-13937.xml\n",
            "model.pt\t\t\t  NEWS2012-Training-EnKa-11955.xml\n",
            "NEWS2012-Training-EnBa-14623.xml  NEWS2012-Training-EnMa-9000.xml\n",
            "NEWS2012-Training-EnHe-11501.xml  NEWS2012-Training-EnTa-11957.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuTb7hNX83hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cff0c4-23b5-4fa9-cb5b-609f679352d5"
      },
      "source": [
        "modAttentionObj.load_state_dict(torch.load('model_attention.pt',map_location=torch.device('cpu')))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ2LNpTW7dGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f75d2c-87bc-4bc9-ef61-c91d18b3c641"
      },
      "source": [
        "def test(net,data,tar_len,device='cpu'):\n",
        "  key,val = list(english_alphabets_indexed.keys()),english_alphabets_indexed.values()\n",
        "  net.eval().to(device)\n",
        "  outputs = net(data,tar_len,trainFlag=False)\n",
        "  convertedList = [[] for i in range(outputs[0].size()[0])]\n",
        "  for eTensor in outputs:\n",
        "    indexes = torch.argmax(eTensor,dim=1).tolist()\n",
        "    strr = ''\n",
        "    for i,index in enumerate(indexes):\n",
        "      strr = key[index]\n",
        "      convertedList[i].append(strr)\n",
        "  return convertedList\n",
        "\n",
        "dataSet = TextLoader(xmlFile='/gdrive/MyDrive/Capstone_project_data/cloned-repo/Transliteration-Indian-Languages/Original-NEWS2012-data/Training/NEWS2012-Training-EnHi-13937.xml')\n",
        "engList,hindiList = dataSet._return_batch_words(10)\n",
        "key = list(hindi_alphabets_indexed.keys())\n",
        "\n",
        "i=0\n",
        "for eWord,hWord in zip(engList,hindiList):\n",
        "  inputs,targets = word_rep(hWord,hindi_alphabets_indexed),gt_rep(eWord,english_alphabets_indexed)\n",
        "  pred = test(modAttentionObj,inputs,len(targets))\n",
        "  tempList = []\n",
        "  for eTensor in inputs:\n",
        "    maxIndex = torch.argmax(eTensor,dim=1)\n",
        "    tempList.append(key[int(maxIndex.item())])\n",
        "\n",
        "  for eList in pred:\n",
        "    ss = ''\n",
        "    for eChar in eList:\n",
        "      if eChar!='PAD':ss+=eChar\n",
        "  bb = ''\n",
        "  for eChar in tempList:\n",
        "    if eChar!='PAD':bb+=eChar\n",
        "  print(ss,'------->>>',bb)\n",
        "  i+=1\n",
        "  if i>5:break\n",
        "\n",
        "  "
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DIL ------->>> दिल\n",
            "MASINA ------->>> मसीना\n",
            "BARKAR ------->>> बरकर\n",
            "BARKI ------->>> बर्कि\n",
            "FE ------->>> फे\n",
            "AMEEN ------->>> अमीन\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFnfWh3kslPj"
      },
      "source": [
        "# Loading Test Data & Computing Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A54E_OCyI4yp"
      },
      "source": [
        "def calc_accuracy(net, device = 'cpu'):\n",
        "  net = net.eval().to(device)\n",
        "  predictions = []\n",
        "  accuracy = 0\n",
        "  for i in range(len(test_data)):\n",
        "      eng, hindi = test_data[i]\n",
        "      gt = gt_rep(hindi, hindi_alpha2index, device)\n",
        "      outputs = infer(net, eng, gt.shape[0], device)\n",
        "      correct = 0\n",
        "      for index, out in enumerate(outputs):\n",
        "          val, indices = out.topk(1)\n",
        "          hindi_pos = indices.tolist()[0]\n",
        "          if hindi_pos[0] == gt[index][0]:\n",
        "              correct += 1\n",
        "      \n",
        "      accuracy += correct/gt.shape[0]\n",
        "  accuracy /= len(test_data)\n",
        "  return accuracy"
      ],
      "execution_count": 76,
      "outputs": []
    }
  ]
}