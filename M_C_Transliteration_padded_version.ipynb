{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "M/C_Transliteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDGqZ/T7rWPzM/PCdLCHIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiraman/Capstone_Project/blob/main/M_C_Transliteration_padded_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tvoTXVpJeVN"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFVwbpaYJdl1"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import string,re\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVu7OSzhDREV"
      },
      "source": [
        "# Load Data from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY-Im6k5DLQM",
        "outputId": "cb2373c9-04ee-4878-95f0-80eab11837cc"
      },
      "source": [
        "!git clone -l -s git://github.com/GokulNC/NLP-Exercises cloned-repo"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Total 72 (delta 0), reused 0 (delta 0), pack-reused 72\u001b[K\n",
            "Receiving objects: 100% (72/72), 2.39 MiB | 5.42 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5LVtrQNHyFP",
        "outputId": "3e94ca66-5a38-49f1-dcbc-b64b2f9469fa"
      },
      "source": [
        "%cd cloned-repo"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-repo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa01lfLNIBbu",
        "outputId": "4cf7d746-c1f6-480f-948a-6ebb2f2793e9"
      },
      "source": [
        "%cd Transliteration-Indian-Languages/Original-NEWS2012-data/Training\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-repo/Transliteration-Indian-Languages/Original-NEWS2012-data/Training\n",
            "NEWS2012-Training-EnBa-14623.xml  NEWS2012-Training-EnKa-11955.xml\n",
            "NEWS2012-Training-EnHe-11501.xml  NEWS2012-Training-EnMa-9000.xml\n",
            "NEWS2012-Training-EnHi-13937.xml  NEWS2012-Training-EnTa-11957.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq7BP0iIZy_u",
        "outputId": "c930d718-6e2f-47fb-c32c-6e28086808f5"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  MyDevice = 'cuda'\n",
        "else:MyDevice = 'cpu'\n",
        "print(MyDevice)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTyZiCO9xEZE"
      },
      "source": [
        "Getting all Hindi & English letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq2ssxlxa4IZ",
        "outputId": "aebe25de-fae4-483a-e384-b80f6e0907f9"
      },
      "source": [
        "## Get all hindi consonants ##\n",
        "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "pad = \"PAD\"\n",
        "hindi_alphabets = [pad]+[chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabets_indexed = {hindi_alphabets[i]:i for i in range(len(hindi_alphabets))}\n",
        "print(hindi_alphabets_indexed)\n",
        "\n",
        "english_alphabets = string.ascii_uppercase\n",
        "english_alphabets_indexed = {}\n",
        "english_alphabets_indexed[pad]=0\n",
        "for ind,char in enumerate(english_alphabets,start=1):\n",
        "  english_alphabets_indexed[char] = ind\n",
        "print(len(english_alphabets_indexed))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'PAD': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF41Hul2xMhk"
      },
      "source": [
        "Clean String Lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI3hfqWgerAR"
      },
      "source": [
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "def _cleanEnglishWord(line):\n",
        "  line = line.replace('-',' ').replace(',',' ').upper()\n",
        "  line = non_eng_letters_regex.sub('', line)\n",
        "  return line.split()\n",
        "\n",
        "def _cleanLanguageWord(line):\n",
        "  line = line.replace('-',' ').replace(',',' ')\n",
        "  cleanedStr = ''\n",
        "  for eChar in line:\n",
        "    if eChar in  hindi_alphabets or eChar in ' ':\n",
        "      cleanedStr+=eChar\n",
        "  return cleanedStr.split()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRuvAaLK9JUQ"
      },
      "source": [
        "# Custom Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF2DbN8AIOgN"
      },
      "source": [
        "class TextLoader(Dataset):\n",
        "  def __init__(self,xmlFile=None):\n",
        "    super().__init__()\n",
        "    self.fileName = xmlFile\n",
        "    self.allEngWords,self.allHindiWords = [],[]\n",
        "    self._read_clean_data()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.allEngWords)\n",
        "\n",
        "  def _read_clean_data(self):\n",
        "    tree = ET.parse(self.fileName)\n",
        "    root = tree.getroot()\n",
        "    for child in root:\n",
        "      engWord = _cleanEnglishWord(child[0].text)\n",
        "      hindWord = _cleanLanguageWord(child[1].text)\n",
        "      if len(engWord)!=len(hindWord):\n",
        "        print(\"Skipping --> {} --- {}\".format(child[0].text,child[1].text))\n",
        "      for eWord in engWord:\n",
        "        self.allEngWords.append(eWord)\n",
        "      for eWord in hindWord:\n",
        "        self.allHindiWords.append(eWord)\n",
        "  def __getitem__(self,idx):\n",
        "    return {\"EnglishWord\":self.allEngWords[idx],\"HindiWord\":self.allHindiWords[idx]}\n",
        "\n",
        "dataSet = TextLoader(xmlFile='NEWS2012-Training-EnHi-13937.xml')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y3ai8-CJpEK",
        "outputId": "2c3337a4-ebe9-4bff-c5a8-2f0adff99a6c"
      },
      "source": [
        "for ind,i in enumerate(dataSet):\n",
        "  if ind>5:break\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'EnglishWord': 'RAASAVIHAAREE', 'HindiWord': 'रासविहारी'}\n",
            "{'EnglishWord': 'DEOGAN', 'HindiWord': 'देवगन'}\n",
            "{'EnglishWord': 'ROAD', 'HindiWord': 'रोड'}\n",
            "{'EnglishWord': 'SHATRUMARDAN', 'HindiWord': 'शत्रुमर्दन'}\n",
            "{'EnglishWord': 'MAHIJUBA', 'HindiWord': 'महिजुबा'}\n",
            "{'EnglishWord': 'SABINE', 'HindiWord': 'सैबिन'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAMgSe9QPfjp"
      },
      "source": [
        "class CustomWordLoader():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def indexHindiWords(self,HindiWordList,maxCharWord,device='cpu'):\n",
        "    finalTensor = torch.zeros(len(HindiWordList),maxCharWord+1,len(hindi_alphabets_indexed))\n",
        "    for wordIndex,eWord in enumerate(sorted(HindiWordList,reverse=True)):\n",
        "      for charIndex,eChar in enumerate(eWord):\n",
        "        pos = hindi_alphabets_indexed.get(eChar)\n",
        "        finalTensor[wordIndex][charIndex][pos]=1\n",
        "    return finalTensor.permute(1,0,2).to(device)\n",
        "\n",
        "  \n",
        "  def indexEnglishWords(self,EnglishWordList,maxCharWord,device='cpu'):\n",
        "    finalTensor = torch.zeros(len(EnglishWordList),maxCharWord+1,1) ## (BatchSize,max_str_len,1)\n",
        "    for wIndex,eWord in enumerate(EnglishWordList):\n",
        "      for Cindex,eChar in enumerate(eWord):\n",
        "        pos = english_alphabets_indexed.get(eChar)\n",
        "        finalTensor[wIndex][Cindex]=pos\n",
        "    return finalTensor.to(device)\n",
        "  \n",
        "  \n",
        "  def returnPackedData(self):\n",
        "    def _getMaxStrLen(wordList):\n",
        "      return max([len(i) for i in wordList])\n",
        "    finalDict = {}\n",
        "    clubbedList = list(((eDict['EnglishWord'],eDict['HindiWord']) for eDict in self.batch))\n",
        "    englishList,HindiList = list(zip(*clubbedList))\n",
        "    engMaxChar,hinMaxChar = _getMaxStrLen(englishList),_getMaxStrLen(HindiList)\n",
        "    OHE_inputs,Targets = self.indexHindiWords(HindiList,hinMaxChar,device=MyDevice),self.indexEnglishWords(englishList,engMaxChar,device=MyDevice)\n",
        "    return {\"Inputs\":OHE_inputs,\"Targets\":Targets}\n",
        "  def __call__(self,batch):\n",
        "    self.batch =batch\n",
        "    return self.returnPackedData()\n",
        "\n",
        "dataLoader = DataLoader(dataSet,batch_size=1,shuffle=True,collate_fn=CustomWordLoader())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhZi-sBBImt7",
        "outputId": "a743c57c-03e0-4e7b-d7af-f53b34f1abaa"
      },
      "source": [
        "for ind,data in enumerate(dataLoader):\n",
        "  if ind>0:break\n",
        "  print(data[\"Inputs\"],data[\"Inputs\"].size())\n",
        "  print(data[\"Targets\"].size(),data[\"Targets\"])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0') torch.Size([5, 1, 129])\n",
            "torch.Size([1, 5, 1]) tensor([[[ 8.],\n",
            "         [ 1.],\n",
            "         [25.],\n",
            "         [15.],\n",
            "         [ 0.]]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiXYzmhp9lDy"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC-8a_9M9qy6",
        "outputId": "a73beecd-eade-40c9-d28d-cad0ca479045"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zMNAcvQ92t-",
        "outputId": "30fa0db7-3f61-40dc-f2db-a08865fccd4b"
      },
      "source": [
        "%cd /gdrive/MyDrive/Capstone_project_data/ImgtoText\n",
        "!ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Capstone_project_data/ImgtoText\n",
            " Annotations   Cropped_Images  'Sample Train'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRGKOd1JCBfj"
      },
      "source": [
        "Enoder Decoder W/O Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCxj2j59R-H"
      },
      "source": [
        "class Encoder_Decoder(nn.Module):\n",
        "  def __init__(self,inputSize,hiddenSize,outputSize,num_layers =1,num_dirns=1,verbose=True):\n",
        "    super().__init__()\n",
        "    self.hiddenSize = hiddenSize\n",
        "    self.outputSize = outputSize\n",
        "    self.num_layers = num_layers\n",
        "    self.num_dirns = num_dirns\n",
        "    self.encoder_GRU = nn.GRU(inputSize,hiddenSize)\n",
        "    self.decoder_GRU = nn.GRU(outputSize,hiddenSize)\n",
        "    self.h2o = nn.Linear(hiddenSize,outputSize)\n",
        "    self.F = nn.LogSoftmax(dim=2)\n",
        "    self.Fll = nn.Softmax(dim=2)\n",
        "    self.verbose = verbose\n",
        "  \n",
        "  def forward(self,inputs,maxCharLen,GT=None,trainFlag =True,device='cpu'):\n",
        "    all_hidden,last_hidden = self.encoder_GRU (inputs)\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"Encoder Input : \",inputs.size())\n",
        "      print(\"Encoder All Hidden Outputs : \",all_hidden.size())\n",
        "      print(\"Encoder Last Hidden Output : \",last_hidden.size())\n",
        "\n",
        "\n",
        "    decoder_state = last_hidden\n",
        "    decoderInput = torch.zeros(1,all_hidden.size()[1],self.outputSize).to(device) ##(1,batchSize,no.of English Alphabets)\n",
        "    if self.verbose:\n",
        "      print(\"Decoder Input : \",decoderInput.size())\n",
        "    \n",
        "    if GT!=None:\n",
        "      GT_trans = torch.transpose(GT,1,0)\n",
        "\n",
        "    outputlist = []   \n",
        "    for i in range(1,maxCharLen):\n",
        "      out,decoder_state = self.decoder_GRU(decoderInput,decoder_state)\n",
        "      output = self.F(self.h2o(decoder_state))\n",
        "      if trainFlag:\n",
        "        outputlist.append(output.squeeze(0))\n",
        "      else:\n",
        "        output_eval = self.Fll(self.h2o(out))\n",
        "        outputlist.append(output_eval.squeeze(0))\n",
        "      if self.verbose:\n",
        "        print(\"Decoder Ouput : \",output.size())\n",
        "        print(\"Squeezed Final Output : \",output.squeeze(0).size())\n",
        "        \n",
        "\n",
        "      maxIndexes = torch.argmax(output,dim=2,keepdim=True).type(torch.int64)\n",
        "      if GT!=None:\n",
        "        maxIndexes = GT_trans[i].reshape(1,all_hidden.size()[1],1).type(torch.int64)\n",
        "\n",
        "      one_hot = torch.FloatTensor(output.size()).to(device)\n",
        "      one_hot.zero_()\n",
        "      one_hot.scatter_(2,maxIndexes,1)\n",
        "      decoderInput = one_hot.detach()\n",
        "    return outputlist\n",
        "      \n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UpaT0WOH9Ey"
      },
      "source": [
        "def trainBatch(modelObj,batchSize,optFn,LossFn,enforceTrain=False,device='cpu'):\n",
        "  dataLoader = DataLoader(dataSet,batch_size=batchSize,shuffle=True,collate_fn=CustomWordLoader())\n",
        "\n",
        "  total_loss = 0\n",
        "  batch_counter = 0\n",
        "  for ind,data in enumerate(dataLoader):\n",
        "    targets = data[\"Targets\"].squeeze(2)\n",
        "    if data[\"Inputs\"].size()[1]!=batchSize:continue\n",
        "    batch_counter+=1\n",
        "    pred_ouputs = modelObj(data[\"Inputs\"],targets.size()[1],GT= targets if enforceTrain else None,device=device)\n",
        "    targets_trans = torch.transpose(targets,1,0).type(torch.LongTensor).to(device)\n",
        "\n",
        "    for index,ouputs in enumerate(pred_ouputs):\n",
        "      loss = LossFn(ouputs,targets_trans[index])/batchSize ## Loss per word\n",
        "      loss.backward(retain_graph=True)\n",
        "      total_loss+=loss.item()         ## Total Loss per batch\n",
        "    \n",
        "    if enforceTrain and len(dataLoader)/(ind +1)==len(dataLoader)//3:break  ## Not allowing to over fit the data \n",
        "\n",
        "  return total_loss/batch_counter   ## Total Loss per Epoch\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxSd_pnaGt12"
      },
      "source": [
        "def training_helper(net,lr=0.5,batch_size=100,epochs=11,momentum = 0.9, display_freq=5, device = 'cpu'):\n",
        "  net.to(device)\n",
        "  lossFn = nn.NLLLoss()\n",
        "  optFn = optim.Adam(modelObj.parameters(),lr=lr)\n",
        "  enforce_Till = epochs//3\n",
        "  sheduler = optim.lr_scheduler.StepLR(optFn,step_size=200,gamma=0.5)\n",
        "\n",
        "  loss_per_epoch_array = torch.zeros(epochs+1)\n",
        "  minVal= 1000000\n",
        "  for i in range(epochs):\n",
        "    optFn.zero_grad()\n",
        "    loss_per_epoch_array[i+1] = (loss_per_epoch_array[i]*i + trainBatch(net, batch_size,optFn, lossFn, device = device, enforceTrain=True if i<enforce_Till else False ))/(i + 1)\n",
        "    optFn.step()\n",
        "    sheduler.step()\n",
        "    \n",
        "    if sheduler.get_lr()!=sheduler.get_last_lr():\n",
        "      print(sheduler.get_lr())\n",
        "    if loss_per_epoch_array[i]<minVal and i>0:\n",
        "      minVal = loss_per_epoch_array[i]\n",
        "      torch.save(net,'model_batched.pt')\n",
        "\n",
        "    if i%display_freq == 0 and i!=0: ## Every 5 epochs refresh the loss plot ##\n",
        "      clear_output(wait=True)\n",
        "      print(\"For Epoch {} ----> Loss {}\".format(i,loss_per_epoch_array[i]))\n",
        "      plt.figure()\n",
        "      plt.plot(loss_per_epoch_array[1:i],'-*')\n",
        "      plt.xlabel(\"Epochs\")\n",
        "      plt.ylabel(\"Epoch Loss\")\n",
        "      plt.show()\n",
        "  return loss_per_epoch_array\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7cucnEpHev0"
      },
      "source": [
        "## HyperParameters ##\n",
        "hiddensize = 256\n",
        "lr = 0.005\n",
        "momentum = 0.9\n",
        "batch_size=64"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VJqSyWSGrRP",
        "outputId": "758fbba5-4e24-4c00-fa4a-6896460175a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "modelObj = Encoder_Decoder(len(hindi_alphabets_indexed),hiddensize,len(english_alphabets_indexed),verbose=False)\n",
        "torch.load('model_batched.pt')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder_Decoder(\n",
              "  (encoder_GRU): GRU(129, 256)\n",
              "  (decoder_GRU): GRU(27, 256)\n",
              "  (h2o): Linear(in_features=256, out_features=27, bias=True)\n",
              "  (F): LogSoftmax(dim=2)\n",
              "  (Fll): Softmax(dim=2)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow2qfeaD5Tn4"
      },
      "source": [
        "training_helper(modelObj,lr=lr, momentum = momentum,batch_size=batch_size,epochs=500,device=MyDevice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuTb7hNX83hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99345ffa-0052-4cca-e276-6c38e24fc899"
      },
      "source": [
        "torch.load('model_batched.pt')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder_Decoder(\n",
              "  (encoder_GRU): GRU(129, 256)\n",
              "  (decoder_GRU): GRU(27, 256)\n",
              "  (h2o): Linear(in_features=256, out_features=27, bias=True)\n",
              "  (F): LogSoftmax(dim=2)\n",
              "  (Fll): Softmax(dim=2)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ2LNpTW7dGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef4f7b9-e4be-4864-b6bf-6890b07f48ed"
      },
      "source": [
        "def test(net,data,device='cpu'):\n",
        "  key,val = list(english_alphabets_indexed.keys()),english_alphabets_indexed.values()\n",
        "  net.eval().to(device)\n",
        "  outputs = net(data[\"Inputs\"].to(device),data[\"Targets\"].size()[1],trainFlag=False)\n",
        "  convertedList = [[] for i in range(outputs[0].size()[0])]\n",
        "  for eTensor in outputs:\n",
        "    indexes = torch.argmax(eTensor,dim=1).tolist()\n",
        "    strr = ''\n",
        "    for i,index in enumerate(indexes):\n",
        "      strr = key[index]\n",
        "      convertedList[i].append(strr)\n",
        "  return convertedList\n",
        "\n",
        "\n",
        "testLoader = DataLoader(dataSet,batch_size=1,shuffle=True,collate_fn=CustomWordLoader())\n",
        "key = list(hindi_alphabets_indexed.keys())\n",
        "\n",
        "i=0\n",
        "for data in testLoader:\n",
        "  pred = test(modelObj,data)\n",
        "  tempList = []\n",
        "  for eTensor in data[\"Inputs\"]:\n",
        "    index = torch.argmax(eTensor,dim=1)\n",
        "    tempList.append(key[int(index.item())])\n",
        "  print(pred,tempList)\n",
        "  i+=1\n",
        "  if i>5:break\n",
        "\n",
        "  "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['A', 'A', 'A', 'PAD', 'PAD']] ['भ', 'ा', 'ल', 'ी', 'PAD']\n",
            "[['A', 'A', 'A']] ['ड', 'ै', 'म', 'PAD']\n",
            "[['A', 'A', 'A', 'A', 'PAD', 'PAD', 'PAD']] ['च', 'ा', 'ल', 'ी', 'स', 'ा', 'PAD']\n",
            "[['A', 'A', 'A', 'PAD']] ['स', 'े', 'व', 'ा', 'PAD']\n",
            "[['A', 'A', 'A', 'A']] ['क', 'ि', 'ट', '्', 'ट', 'ी', 'PAD']\n",
            "[['A', 'A', 'A', 'A', 'PAD']] ['फ', '्', 'र', 'ि', 'क', 'PAD']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-YS2j7if2QQ"
      },
      "source": [
        "batchSize = 1\n",
        "testSet = TextLoader(xmlFile='NEWS2012-Testing-EnHi-1000.xml')\n",
        "data = DataLoader(dataSet,batch_size=batchSize,shuffle=True,collate_fn=CustomWordLoader())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv8GTwaxl9Aj"
      },
      "source": [
        "aa = [1,2,3,5,6]\n",
        "b= 0\n",
        "if b<"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}