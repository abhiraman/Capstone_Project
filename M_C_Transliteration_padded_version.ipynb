{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "M/C_Transliteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9iSBDOY40dkYBtkHLQbh2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiraman/Capstone_Project/blob/main/M_C_Transliteration_padded_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tvoTXVpJeVN"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFVwbpaYJdl1"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import string,re\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVu7OSzhDREV"
      },
      "source": [
        "# Load Data from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY-Im6k5DLQM",
        "outputId": "740b9732-20dc-4fc8-b98c-9cf305cc39e1"
      },
      "source": [
        "!git clone -l -s git://github.com/GokulNC/NLP-Exercises cloned-repo"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Total 72 (delta 0), reused 0 (delta 0), pack-reused 72\u001b[K\n",
            "Receiving objects: 100% (72/72), 2.39 MiB | 5.16 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5LVtrQNHyFP",
        "outputId": "7581d4b0-5d96-487a-99c0-80d96a54978d"
      },
      "source": [
        "%cd cloned-repo"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-repo/Transliteration-Indian-Languages/Original-NEWS2012-data/Training/cloned-repo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa01lfLNIBbu",
        "outputId": "d8845bdf-aa0c-408e-ab4f-b88f351151f3"
      },
      "source": [
        "%cd Transliteration-Indian-Languages/Original-NEWS2012-data/Training\n",
        "!ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cloned-repo/Transliteration-Indian-Languages/Original-NEWS2012-data/Training/cloned-repo/Transliteration-Indian-Languages/Original-NEWS2012-data/Training\n",
            "NEWS2012-Training-EnBa-14623.xml  NEWS2012-Training-EnKa-11955.xml\n",
            "NEWS2012-Training-EnHe-11501.xml  NEWS2012-Training-EnMa-9000.xml\n",
            "NEWS2012-Training-EnHi-13937.xml  NEWS2012-Training-EnTa-11957.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq7BP0iIZy_u",
        "outputId": "53fc10b4-906e-4ae2-b695-f4c50cb58216"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  MyDevice = 'cuda'\n",
        "else:MyDevice = 'cpu'\n",
        "print(MyDevice)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTyZiCO9xEZE"
      },
      "source": [
        "Getting all Hindi & English letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq2ssxlxa4IZ",
        "outputId": "3f9e44c4-9e1f-4371-936b-316f2d9ae0e8"
      },
      "source": [
        "## Get all hindi consonants ##\n",
        "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "pad = \"PAD\"\n",
        "hindi_alphabets = [pad]+[chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabets_indexed = {hindi_alphabets[i]:i for i in range(len(hindi_alphabets))}\n",
        "print(hindi_alphabets_indexed)\n",
        "\n",
        "english_alphabets = string.ascii_uppercase\n",
        "english_alphabets_indexed = {}\n",
        "english_alphabets_indexed[pad]=0\n",
        "for ind,char in enumerate(english_alphabets,start=1):\n",
        "  english_alphabets_indexed[char] = ind\n",
        "print(len(english_alphabets_indexed))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'PAD': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF41Hul2xMhk"
      },
      "source": [
        "Clean String Lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI3hfqWgerAR"
      },
      "source": [
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "def _cleanEnglishWord(line):\n",
        "  line = line.replace('-',' ').replace(',',' ').upper()\n",
        "  line = non_eng_letters_regex.sub('', line)\n",
        "  return line.split()\n",
        "\n",
        "def _cleanLanguageWord(line):\n",
        "  line = line.replace('-',' ').replace(',',' ')\n",
        "  cleanedStr = ''\n",
        "  for eChar in line:\n",
        "    if eChar in  hindi_alphabets or eChar in ' ':\n",
        "      cleanedStr+=eChar\n",
        "  return cleanedStr.split()\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRuvAaLK9JUQ"
      },
      "source": [
        "# Custom Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF2DbN8AIOgN"
      },
      "source": [
        "class TextLoader(Dataset):\n",
        "  def __init__(self,xmlFile=None):\n",
        "    super().__init__()\n",
        "    self.fileName = xmlFile\n",
        "    self.allEngWords,self.allHindiWords = [],[]\n",
        "    self._read_clean_data()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.allEngWords)\n",
        "\n",
        "  def _read_clean_data(self):\n",
        "    tree = ET.parse(self.fileName)\n",
        "    root = tree.getroot()\n",
        "    for child in root:\n",
        "      engWord = _cleanEnglishWord(child[0].text)\n",
        "      hindWord = _cleanLanguageWord(child[1].text)\n",
        "      if len(engWord)!=len(hindWord):\n",
        "        print(\"Skipping --> {} --- {}\".format(child[0].text,child[1].text))\n",
        "      for eWord in engWord:\n",
        "        self.allEngWords.append(eWord)\n",
        "      for eWord in hindWord:\n",
        "        self.allHindiWords.append(eWord)\n",
        "  def __getitem__(self,idx):\n",
        "    return {\"EnglishWord\":self.allEngWords[idx],\"HindiWord\":self.allHindiWords[idx]}\n",
        "\n",
        "dataSet = TextLoader(xmlFile='NEWS2012-Training-EnHi-13937.xml')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y3ai8-CJpEK",
        "outputId": "2c3337a4-ebe9-4bff-c5a8-2f0adff99a6c"
      },
      "source": [
        "for ind,i in enumerate(dataSet):\n",
        "  if ind>5:break\n",
        "  print(i)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'EnglishWord': 'RAASAVIHAAREE', 'HindiWord': 'रासविहारी'}\n",
            "{'EnglishWord': 'DEOGAN', 'HindiWord': 'देवगन'}\n",
            "{'EnglishWord': 'ROAD', 'HindiWord': 'रोड'}\n",
            "{'EnglishWord': 'SHATRUMARDAN', 'HindiWord': 'शत्रुमर्दन'}\n",
            "{'EnglishWord': 'MAHIJUBA', 'HindiWord': 'महिजुबा'}\n",
            "{'EnglishWord': 'SABINE', 'HindiWord': 'सैबिन'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAMgSe9QPfjp"
      },
      "source": [
        "class CustomWordLoader():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def indexHindiWords(self,HindiWordList,maxCharWord,device='cpu'):\n",
        "    finalTensor = torch.zeros(len(HindiWordList),maxCharWord,len(hindi_alphabets_indexed))\n",
        "    for wordIndex,eWord in enumerate(sorted(HindiWordList,reverse=True)):\n",
        "      for charIndex,eChar in enumerate(eWord):\n",
        "        pos = hindi_alphabets_indexed.get(eChar)\n",
        "        finalTensor[wordIndex][charIndex][pos]=1\n",
        "    return finalTensor.permute(1,0,2).to(device)\n",
        "\n",
        "  \n",
        "  def indexEnglishWords(self,EnglishWordList,maxCharWord,device='cpu'):\n",
        "    finalTensor = torch.zeros(len(EnglishWordList),maxCharWord+1,1) ## (BatchSize,max_str_len,1)\n",
        "    for wIndex,eWord in enumerate(EnglishWordList):\n",
        "      finalTensor[wIndex][0] = 0\n",
        "      for Cindex,eChar in enumerate(eWord,start=1):\n",
        "        pos = english_alphabets_indexed.get(eChar)\n",
        "        finalTensor[wIndex][Cindex]=pos\n",
        "    return finalTensor.to(device)\n",
        "  \n",
        "  \n",
        "  def returnPackedData(self):\n",
        "    def _getMaxStrLen(wordList):\n",
        "      return max([len(i) for i in wordList])\n",
        "    finalDict = {}\n",
        "    clubbedList = list(((eDict['EnglishWord'],eDict['HindiWord']) for eDict in self.batch))\n",
        "    englishList,HindiList = list(zip(*clubbedList))\n",
        "    engMaxChar,hinMaxChar = _getMaxStrLen(englishList),_getMaxStrLen(HindiList)\n",
        "    OHE_inputs,Targets = self.indexHindiWords(HindiList,hinMaxChar,device=MyDevice),self.indexEnglishWords(englishList,engMaxChar,device=MyDevice)\n",
        "    return {\"Inputs\":OHE_inputs,\"Targets\":Targets}\n",
        "  def __call__(self,batch):\n",
        "    self.batch =batch\n",
        "    return self.returnPackedData()\n",
        "\n",
        "dataLoader = DataLoader(dataSet,batch_size=1,shuffle=True,collate_fn=CustomWordLoader())"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhZi-sBBImt7",
        "outputId": "a67f19b4-69a3-437b-8617-c80364d957bd"
      },
      "source": [
        "for ind,data in enumerate(dataLoader):\n",
        "  if ind>0:break\n",
        "  print(data[\"Inputs\"],data[\"Inputs\"].size())\n",
        "  print(data[\"Targets\"].size(),data[\"Targets\"])\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0') torch.Size([4, 1, 129])\n",
            "torch.Size([1, 5, 1]) tensor([[[22.],\n",
            "         [ 9.],\n",
            "         [14.],\n",
            "         [ 1.],\n",
            "         [25.]]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiXYzmhp9lDy"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC-8a_9M9qy6",
        "outputId": "a40523e5-dee2-4cbe-913e-176627023bd2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zMNAcvQ92t-",
        "outputId": "f0f05e62-959e-4a71-8e13-57f493f2707b"
      },
      "source": [
        "%cd /gdrive/MyDrive/Capstone_project_data\n",
        "!ls"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Capstone_project_data\n",
            " cloned-repo  'Synthetic Train Set - Detection & Recognition'\n",
            " ImgtoText    'Synthetic Train Set - Detection & Recognition.zip'\n",
            " model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRGKOd1JCBfj"
      },
      "source": [
        "Enoder Decoder W/O Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCxj2j59R-H"
      },
      "source": [
        "class Encoder_Decoder(nn.Module):\n",
        "  def __init__(self,inputSize,hiddenSize,outputSize,num_layers =1,num_dirns=1,verbose=True):\n",
        "    super().__init__()\n",
        "    self.hiddenSize = hiddenSize\n",
        "    self.outputSize = outputSize\n",
        "    self.num_layers = num_layers\n",
        "    self.num_dirns = num_dirns\n",
        "    self.encoder_GRU = nn.GRU(inputSize,hiddenSize)\n",
        "    self.decoder_GRU = nn.GRU(outputSize,hiddenSize)\n",
        "    self.h2o = nn.Linear(hiddenSize,outputSize)\n",
        "    self.F = nn.LogSoftmax(dim=2)\n",
        "    self.Fll = nn.Softmax(dim=2)\n",
        "    self.verbose = verbose\n",
        "  \n",
        "  def forward(self,inputs,maxCharLen,GT=None,trainFlag =True,device='cpu'):\n",
        "    all_hidden,last_hidden = self.encoder_GRU (inputs)\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"Encoder Input : \",inputs.size())\n",
        "      print(\"Encoder All Hidden Outputs : \",all_hidden.size())\n",
        "      print(\"Encoder Last Hidden Output : \",last_hidden.size())\n",
        "\n",
        "\n",
        "    decoder_state = last_hidden\n",
        "    decoderInput = torch.zeros(1,all_hidden.size()[1],self.outputSize).to(device) ##(1,batchSize,no.of English Alphabets)\n",
        "    if self.verbose:\n",
        "      print(\"Decoder Input : \",decoderInput.size())\n",
        "    \n",
        "    if GT!=None:\n",
        "      GT_trans = torch.transpose(GT,1,0)\n",
        "\n",
        "    outputlist = []   \n",
        "    for i in range(1,maxCharLen):\n",
        "      out,decoder_state = self.decoder_GRU(decoderInput,decoder_state)\n",
        "      output = self.F(self.h2o(out))\n",
        "      if trainFlag:\n",
        "        outputlist.append(output.squeeze(0))\n",
        "      else:\n",
        "        output_eval = self.Fll(self.h2o(out))\n",
        "        outputlist.append(output_eval.squeeze(0))\n",
        "      if self.verbose:\n",
        "        print(\"Decoder Ouput : \",output.size())\n",
        "        print(\"Squeezed Final Output : \",output.squeeze(0).size())\n",
        "        \n",
        "\n",
        "      maxIndexes = torch.argmax(output,dim=2,keepdim=True).type(torch.int64)\n",
        "      if GT!=None:\n",
        "        maxIndexes = GT_trans[i].reshape(1,all_hidden.size()[1],1).type(torch.int64)\n",
        "\n",
        "      one_hot = torch.FloatTensor(output.size()).to(device)\n",
        "      one_hot.zero_()\n",
        "      one_hot.scatter_(2,maxIndexes,1)\n",
        "      decoderInput = one_hot.detach()\n",
        "    return outputlist\n",
        "      \n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UpaT0WOH9Ey"
      },
      "source": [
        "def trainBatch(modelObj,batchSize,optFn,LossFn,enforceTrain=False,device='cpu'):\n",
        "  dataLoader = DataLoader(dataSet,batch_size=batchSize,shuffle=True,collate_fn=CustomWordLoader())\n",
        "\n",
        "  total_loss = 0\n",
        "  batch_counter = 0\n",
        "  for data in dataLoader:\n",
        "    optFn.zero_grad()\n",
        "    targets = data[\"Targets\"].squeeze(2)\n",
        "    if data[\"Inputs\"].size()[1]!=batchSize:continue\n",
        "    batch_counter+=1\n",
        "    if enforceTrain:pred_ouputs = modelObj(data[\"Inputs\"],targets.size()[1],GT= targets,device=device)\n",
        "    else:pred_ouputs = modelObj(data[\"Inputs\"],targets.size()[1],device=device)\n",
        "    ## Loss & Gradient compute for evry time step ##\n",
        "    targets_trans = torch.transpose(targets,1,0).type(torch.LongTensor).to(device)\n",
        "\n",
        "    for index,ouputs in enumerate(pred_ouputs):\n",
        "      loss = LossFn(ouputs,targets_trans[index])/batchSize ## Loss per word\n",
        "      loss.backward(retain_graph=True)\n",
        "      total_loss+=loss.item()         ## Total Loss per batch\n",
        "    optFn.step() \n",
        "\n",
        "  return total_loss/batch_counter   ## Total Loss per Epoch\n",
        "\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxSd_pnaGt12"
      },
      "source": [
        "def training_helper(net,lr=0.5,batch_size=100,epochs=11,momentum = 0.9, display_freq=5, device = 'cpu'):\n",
        "  net.to(device)\n",
        "  lossFn = nn.NLLLoss()\n",
        "  optFn = optim.Adam(modelObj.parameters(),lr=lr,amsgrad=True)\n",
        "  enforce_Till = epochs//batch_size\n",
        "\n",
        "  loss_per_epoch_array = torch.zeros(epochs+1)\n",
        "  minVal= 1000000\n",
        "  for i in range(epochs):\n",
        "    loss_per_epoch_array[i+1] = trainBatch(net,batch_size,optFn,lossFn,enforceTrain=True if i<=enforce_Till else False,device=device)\n",
        "    #loss_per_epoch_array[i+1] = trainBatch(net,batch_size,optFn,lossFn,enforceTrain=False,device=device)\n",
        "\n",
        "\n",
        "    if loss_per_epoch_array[i]<minVal and i>0:\n",
        "      minVal = loss_per_epoch_array[i]\n",
        "      torch.save(net,'model.pt')\n",
        "\n",
        "    if i%display_freq == 0 and i!=0: ## Every 5 epochs refresh the loss plot ##\n",
        "      clear_output(wait=True)\n",
        "      print(\"For Epoch {} ----> Loss {}\".format(i,loss_per_epoch_array[i]))\n",
        "      plt.figure()\n",
        "      plt.plot(loss_per_epoch_array[1:i],'-*')\n",
        "      plt.xlabel(\"Epochs\")\n",
        "      plt.ylabel(\"Epoch Loss\")\n",
        "      plt.show()\n",
        "  return loss_per_epoch_array\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7cucnEpHev0"
      },
      "source": [
        "## HyperParameters ##\n",
        "hiddensize = 256\n",
        "lr = 0.001\n",
        "momentum = 0.9\n",
        "batch_size=64"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VJqSyWSGrRP"
      },
      "source": [
        "modelObj = Encoder_Decoder(len(hindi_alphabets_indexed),hiddensize,len(english_alphabets_indexed),verbose=False)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "ow2qfeaD5Tn4",
        "outputId": "4660ea46-5f1c-443c-bbd6-78c52c2a76dc"
      },
      "source": [
        "training_helper(modelObj,lr=lr, momentum = momentum,batch_size=batch_size,epochs=100,device=MyDevice)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([13, 64, 129])\n",
            "torch.Size([64, 129])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-1af83398f547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelObj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMyDevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-e4b27f9528a7>\u001b[0m in \u001b[0;36mtraining_helper\u001b[0;34m(net, lr, batch_size, epochs, momentum, display_freq, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mminVal\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss_per_epoch_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptFn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlossFn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menforceTrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0menforce_Till\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#loss_per_epoch_array[i+1] = trainBatch(net,batch_size,optFn,lossFn,enforceTrain=False,device=device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-4480305a3198>\u001b[0m in \u001b[0;36mtrainBatch\u001b[0;34m(modelObj, batchSize, optFn, LossFn, enforceTrain, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meinpTensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meinpTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0;36m0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Inputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuTb7hNX83hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989aa5ea-ba04-4828-9512-cef0845e28be"
      },
      "source": [
        "torch.load('model.pt')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder_Decoder(\n",
              "  (encoder_GRU): GRU(129, 256)\n",
              "  (decoder_GRU): GRU(27, 256)\n",
              "  (h2o): Linear(in_features=256, out_features=27, bias=True)\n",
              "  (F): LogSoftmax(dim=2)\n",
              "  (Fll): Softmax(dim=2)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ2LNpTW7dGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b2e5c3-4ba4-4883-cd9d-4dd197864506"
      },
      "source": [
        "def test(net,data,device='cpu'):\n",
        "  key,val = list(english_alphabets_indexed.keys()),english_alphabets_indexed.values()\n",
        "  net.eval().to(device)\n",
        "  outputs = net(data[\"Inputs\"].to(device),data[\"Targets\"].size()[1],trainFlag=False)\n",
        "  convertedList = [[] for i in range(outputs[0].size()[0])]\n",
        "  for eTensor in outputs:\n",
        "    indexes = torch.argmax(eTensor,dim=1).tolist()\n",
        "    strr = ''\n",
        "    for i,index in enumerate(indexes):\n",
        "      strr = key[index]\n",
        "      convertedList[i].append(strr)\n",
        "  return convertedList\n",
        "\n",
        "\n",
        "testLoader = DataLoader(dataSet,batch_size=1,shuffle=True,collate_fn=CustomWordLoader())\n",
        "key = list(hindi_alphabets_indexed.keys())\n",
        "\n",
        "i=0\n",
        "for data in testLoader:\n",
        "  pred = test(modelObj,data)\n",
        "  tempList = []\n",
        "  for eTensor in data[\"Inputs\"]:\n",
        "    index = torch.argmax(eTensor,dim=1)\n",
        "    tempList.append(key[int(index.item())])\n",
        "  print(pred,tempList)\n",
        "  i+=1\n",
        "  if i>5:break\n",
        "\n",
        "  "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['PAD', 'A', 'A', 'A', 'A']] ['ब', 'ु', 'ं', 'ग', 'ा']\n",
            "[['PAD', 'A', 'A', 'A', 'A', 'PAD']] ['ब', 'ो', 'इ', 'स', 'र']\n",
            "[['PAD', 'A', 'A', 'A', 'A']] ['स', 'त', '्', 'त', 'े']\n",
            "[['PAD', 'A', 'A', 'A', 'A', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']] ['ब', 'े', 'न', '्', 'क', 'न', 'स', '्', 'ट', 'ी', 'न']\n",
            "[['PAD', 'A', 'A', 'R', 'PAD']] ['ए', 'ड', 'व', 'ि', 'न']\n",
            "[['PAD', 'A', 'A', 'R', 'PAD', 'PAD']] ['ब', 'ि', 'श', 'प']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-YS2j7if2QQ"
      },
      "source": [
        "batchSize = 1\n",
        "testSet = TextLoader(xmlFile='NEWS2012-Testing-EnHi-1000.xml')\n",
        "data = DataLoader(dataSet,batch_size=batchSize,shuffle=True,collate_fn=CustomWordLoader())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv8GTwaxl9Aj"
      },
      "source": [
        "aa = [1,2,3,5,6]\n",
        "b= 0\n",
        "if b<"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}